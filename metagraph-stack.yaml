AWSTemplateFormatVersion: '2010-09-09'
Description: 'Deploys an AWS Batch-based system for Metagraph query execution.'

Parameters:
  NotificationEmail:
    Type: String
    Description: Email address to subscribe to the SNS topic


Resources:

### Roles and profiles
  ECSInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - "sts:AssumeRole"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
      Policies:
        - PolicyName: MountS3ReadAccess
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:ListBucket
                  - s3:GetObject
                  - s3:PutObject
                Resource:
                  - arn:aws:s3:::metagraph-data-public
                  - arn:aws:s3:::metagraph-data-public/*
                  - !Sub arn:aws:s3:::${MetagraphBucket}
                  - !Sub arn:aws:s3:::${MetagraphBucket}/*

  ECSInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref ECSInstanceRole

  MetagraphLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: LambdaSubmitBatchJobsPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'batch:SubmitJob'
                  - 'batch:DescribeJobs'
                Resource: "*"
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:GetObject'
                  - 's3:PutObject'
                Resource:
                  - 'arn:aws:s3:::metagraph-data-public*'
                  - !Sub 'arn:aws:s3:::${MetagraphBucket}*'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref MetagraphNotificationTopic

  MetagraphStepFunctionsRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service: states.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: StepFunctionsInvokeLambda
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: lambda:InvokeFunction
                Resource:
                  - !GetAtt MetagraphTaskScheduler.Arn
                  - !GetAtt MetagraphJobMonitor.Arn
                  - !GetAtt MetagraphResultAggregator.Arn
                  - !GetAtt MetagraphNotifier.Arn

  MetagraphBatchJobRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: ecs-tasks.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: MetagraphJobS3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:ListBucket'
                Resource:
                  - 'arn:aws:s3:::metagraph-data-public*'
                  - !Sub 'arn:aws:s3:::${MetagraphBucket}'
                  - !Sub 'arn:aws:s3:::${MetagraphBucket}/*'
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                Resource:
                  - !Sub 'arn:aws:s3:::${MetagraphBucket}/*'

# Profile and role for building AMI
  MetagraphInstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles: [!Ref MetagraphInstanceRole]

  MetagraphInstanceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service: ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
        - arn:aws:iam::aws:policy/AWSImageBuilderFullAccess
        - arn:aws:iam::aws:policy/AmazonS3FullAccess

### Metagraph + Mountpoint AMI
  MountpointS3Component:
    Type: AWS::ImageBuilder::Component
    Properties:
      Name: MountpointS3Setup
      Version: 1.0.3
      Platform: Linux
      Data: |
        name: MountpointS3Setup
        description: Installs mount-s3 and Metagraph
        schemaVersion: 1.0
        phases:
          - name: build
            steps:
              - name: ReleaseUpgrade
                action: ExecuteBash
                inputs:
                  commands:
                    - dnf upgrade --releasever=latest -y
              - name: InstallMountS3
                action: ExecuteBash
                inputs:
                  commands:
                    - yum install -y wget
                    - wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm
                    - yum install -y ./mount-s3.rpm
              - name: InstallDependencies
                action: ExecuteBash
                inputs:
                  commands:
                    - yum install -y git gcc14-c++ cmake boost-devel boost-static zlib-devel bzip2-devel automake autoconf libtool libatomic libatomic-static
                    - export CC=/usr/bin/gcc14-gcc
                    - export CXX=/usr/bin/gcc14-g++
                    - git clone https://github.com/ebiggers/libdeflate.git
                    - mkdir libdeflate/build && cd libdeflate/build
                    - cmake ..
                    - make -j$(nproc)
                    - make install
              - name: CloneMetagraph
                action: ExecuteBash
                inputs:
                  commands:
                    - git clone --recursive https://github.com/ratschlab/metagraph.git /opt/metagraph
              - name: BuildSDSL
                action: ExecuteBash
                inputs:
                  commands:
                    - export CC=/usr/bin/gcc14-gcc
                    - export CXX=/usr/bin/gcc14-g++
                    - pushd /opt/metagraph/metagraph/external-libraries/sdsl-lite
                    - sudo env CC=$CC CXX=$CXX ./install.sh $PWD
                    - popd
              - name: BuildMetagraph
                action: ExecuteBash
                inputs:
                  commands:
                    - export CC=/usr/bin/gcc14-gcc
                    - export CXX=/usr/bin/gcc14-g++
                    - mkdir -p /opt/metagraph/metagraph/build
                    - cd /opt/metagraph/metagraph/build
                    - cmake -DCMAKE_INSTALL_PREFIX=/opt/metagraph/install ..
                    - make -j$(nproc)
                    - make install

  MetagraphInfraConfig:
    Type: AWS::ImageBuilder::InfrastructureConfiguration
    Properties:
      Name: MetagraphInfraConfig
      InstanceTypes: [t3.2xlarge]
      InstanceProfileName: !Ref MetagraphInstanceProfile
      TerminateInstanceOnFailure: true
      Logging:
        S3Logs:
          S3BucketName: !Ref MetagraphBucket

  MetagraphImageRecipe:
    Type: AWS::ImageBuilder::ImageRecipe
    Properties:
      Name: MetagraphImageRecipe
      Version: 1.0.3
      ParentImage: arn:aws:imagebuilder:eu-central-2:aws:image/amazon-linux-2023-ecs-optimized-x86/x.x.x
      Components:
        - ComponentArn: !Ref MountpointS3Component

  MetagraphImagePipeline:
    Type: AWS::ImageBuilder::Image
    Properties:
      ImageRecipeArn: !Ref MetagraphImageRecipe
      InfrastructureConfigurationArn: !Ref MetagraphInfraConfig
      ImageTestsConfiguration:
        ImageTestsEnabled: false

### Main stack bucket
  MetagraphBucket:
    Type: AWS::S3::Bucket

### SNS topic for notifications
  MetagraphNotificationTopic:
    Type: AWS::SNS::Topic

  MetagraphNotificationSubscription:
    Type: AWS::SNS::Subscription
    Properties:
      TopicArn: !Ref MetagraphNotificationTopic
      Protocol: email
      Endpoint: !Ref NotificationEmail

### Launch template for container service with s3 bucket mounting
  MetagraphLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateData:
        IamInstanceProfile:
          Name: !Ref ECSInstanceProfile
        ImageId: !GetAtt MetagraphImagePipeline.ImageId
        UserData:
          Fn::Base64: !Sub |
            MIME-Version: 1.0
            Content-Type: multipart/mixed; boundary="==BOUNDARY=="

            --==BOUNDARY==
            Content-Type: text/cloud-config; charset="us-ascii"

            cloud_final_modules:
              - [scripts-user, always]

            --==BOUNDARY==
            Content-Type: text/x-shellscript; charset="us-ascii"

            #!/bin/bash
            set -e

            mkdir -p /mnt/data
            mkdir -p /mnt/bucket

            mount-s3 metagraph-data-public /mnt/data
            mount-s3 ${MetagraphBucket} /mnt/bucket

            --==BOUNDARY==--


  MetagraphBatchComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
      Type: MANAGED
      State: ENABLED
      ComputeResources:
        Type: EC2
        MinvCpus: 0
        MaxvCpus: 512
        InstanceTypes:
          - r6i.4xlarge # 16 vCPU, 128 GiB RAM
        Subnets:
          - subnet-064a41e7e9f7821a8 # eu-central-2a
          - subnet-0247650ef94e8aa42 # eu-central-2b
          - subnet-02ebd4969f85757c4 # eu-central-2c
        SecurityGroupIds:
          - sg-08aa6d6a56d63b8cd
        InstanceRole: !GetAtt ECSInstanceProfile.Arn
        LaunchTemplate:
          LaunchTemplateId: !Ref MetagraphLaunchTemplate
          Version: !GetAtt MetagraphLaunchTemplate.LatestVersionNumber

  MetagraphBatchJobQueue:
    Type: AWS::Batch::JobQueue
    Properties:
      State: ENABLED
      Priority: 1
      ComputeEnvironmentOrder:
        - Order: 1
          ComputeEnvironment: !Ref MetagraphBatchComputeEnvironment

  MetagraphBatchJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
      Type: container
      RetryStrategy:
        Attempts: 3
      ContainerProperties:
        Image: public.ecr.aws/amazonlinux/amazonlinux:2023
        Vcpus: 4
        Memory: 32768
        Volumes:
          - Name: metagraph-data
            Host:
              SourcePath: /mnt/data
          - Name: metagraph-bucket
            Host:
              SourcePath: /mnt/bucket
          - Name: metagraph-bin
            Host:
              SourcePath: /opt/metagraph
          - Name: libdeflate
            Host:
              SourcePath: /usr/local/lib64
          - Name: lib64
            Host:
              SourcePath: /usr/lib64
        MountPoints:
          - SourceVolume: metagraph-data
            ContainerPath: /mnt/data
          - SourceVolume: metagraph-bucket
            ContainerPath: /mnt/bucket
          - SourceVolume: metagraph-bin
            ContainerPath: /opt/metagraph
          - SourceVolume: libdeflate
            ContainerPath: /usr/local/lib64
          - SourceVolume: lib64
            ContainerPath: /usr/lib64
        Command:
          - "/bin/bash"
          - "-c"
          - |
            set -e

            export PATH="/opt/metagraph/metagraph/build:$PATH"
            export LD_LIBRARY_PATH="/usr/local/lib64:/usr/lib64:$LD_LIBRARY_PATH"

            metagraph query \
              -i "/mnt/data/$DATASET/$GRAPH_FILE" \
              -a "/mnt/data/$DATASET/$ANNO_FILE" \
              --query-mode "$QUERY_MODE" \
              --num-top-labels "$NUM_TOP_LABELS" \
              --min-kmers-fraction-label "$MIN_KMERS_FRACTION_LABEL" \
              --min-kmers-fraction-graph "$MIN_KMERS_FRACTION_GRAPH" \
              "/mnt/bucket/$QUERY" > result.txt

            mkdir -p "/mnt/bucket/$JOB_ID"
            cp result.txt "/mnt/bucket/$JOB_ID/$AWS_BATCH_JOB_ID.txt"
        JobRoleArn: !GetAtt MetagraphBatchJobRole.Arn

  MetagraphTaskScheduler:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt MetagraphLambdaRole.Arn
      Runtime: python3.13
      Handler: index.lambda_handler
      Timeout: 300
      Code:
        ZipFile: !Sub |
          import boto3, json, os, uuid, re, math
          from collections import defaultdict

          s3 = boto3.client('s3')
          batch = boto3.client('batch')

          def list_indices(bucket, prefix):
              prefix = prefix.strip('/').rstrip('/')
              prefix += '/' if prefix else ''
              directories = defaultdict(dict)
              paginator = s3.get_paginator("list_objects_v2")
              for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
                  for element in page.get("Contents", []):
                      key = element['Key']
                      is_graph = key.endswith('.dbg')
                      is_anno = key.endswith('.annodbg')
                      if is_graph or is_anno:
                          splt = key.split('/')
                          directories['/'.join(splt[:-1])[len(prefix):]]['graph' if is_graph else 'anno'] = splt[-1]
              return directories

          def lambda_handler(event, context):
              prefix = event["query"]["index_prefix"].rstrip('/')
              bucket = 'metagraph-data-public'

              all_directories = list_indices(bucket, prefix)
              regex_pattern = event["query"].get("index_filter", ".*")
              print(f'Found {len(all_directories)} indices in {prefix}')
              matching_dirs = [(path, files) for path, files in all_directories.items() if re.match(regex_pattern, path)]
              print(f'{len(matching_dirs)} indices matched against "{regex_pattern}"')

              job_id = event["arn"].split(":")[-1] if "arn" in event else context.aws_request_id
              query_mode = event["query"].get("query_mode", "labels")

              print(f'Assigning tasks for Job ID {job_id}')

              tasks = []
              for relative_path, files in matching_dirs:
                  if "graph" not in files or "anno" not in files:
                      print(f"Missing required files in {relative_path}. Found: {list(files.keys())}")
                      continue

                  task_name = relative_path.replace("/", "_")

                  def file_size(bucket, key): # bytes
                      return s3.head_object(Bucket=bucket, Key=key)["ContentLength"]

                  graph_file = files["graph"]
                  anno_file = files["anno"]
                  graph_size = file_size(bucket, f"{prefix}/{relative_path}/{graph_file}")
                  anno_size = file_size(bucket, f"{prefix}/{relative_path}/{anno_file}")

                  memory_mib = (graph_size + anno_size) // 2**20 + 8*1024 # 8 GiB overhead

                  print(f"Scheduling {task_name} at {memory_mib / 1024} GiB memory")

                  tasks.append(batch.submit_job(
                      jobName=f"MetagraphQuery-{job_id}-{task_name}"[:128],
                      jobQueue="${MetagraphBatchJobQueue}",
                      jobDefinition="${MetagraphBatchJobDefinition}",
                      containerOverrides={
                          "memory": memory_mib,
                          "environment": [
                            {"name": "DATASET", "value": f"{prefix}/{relative_path}"},
                            {"name": "GRAPH_FILE", "value": graph_file},
                            {"name": "ANNO_FILE", "value": anno_file},
                            {"name": "QUERY", "value": f"queries/{event['query']['query_filename']}"},
                            {"name": "JOB_ID", "value": job_id},
                            {"name": "QUERY_MODE", "value": query_mode},
                            {"name": "NUM_TOP_LABELS", "value": str(event["query"].get("num_top_labels", "inf"))},
                            {"name": "MIN_KMERS_FRACTION_LABEL", "value": str(event["query"].get("min_kmers_fraction_label", 0.7))},
                            {"name": "MIN_KMERS_FRACTION_GRAPH", "value": str(event["query"].get("min_kmers_fraction_graph", 0.0))}
                        ]
                      }
                  )['jobId'])

              return {
                  "query_mode": query_mode,
                  "job_id": job_id,
                  "tasks": tasks
              }

  MetagraphJobMonitor:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt MetagraphLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3

          batch = boto3.client('batch')

          def lambda_handler(event, context):
              tasks = event["tasks"]
              
              # Split the task list into chunks of at most 100
              def chunked(iterable, size):
                  for i in range(0, len(iterable), size):
                      yield iterable[i:i + size]

              statuses = []
              for chunk in chunked(tasks, 100):
                  response = batch.describe_jobs(jobs=chunk)
                  statuses.extend(job["status"] for job in response["jobs"])

              return {
                  "query_mode": event["query_mode"],
                  "job_id": event["job_id"],
                  "tasks": tasks,
                  "total": len(statuses),
                  "fails": sum(status == "FAILED" for status in statuses),
                  "all_done": all(status in ("SUCCEEDED", "FAILED") for status in statuses)
              }
      Runtime: python3.13
      Timeout: 60

  MetagraphResultAggregator:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt MetagraphLambdaRole.Arn
      Code:
        ZipFile: !Sub |
          import boto3
          import io
          import itertools as it

          from concurrent.futures import ThreadPoolExecutor

          s3 = boto3.client('s3')

          def lambda_handler(event, context):
              bucket = "${MetagraphBucket}"
              job_id = event["job_id"]

              # List the objects for the current job_id
              result = s3.list_objects_v2(Bucket=bucket, Prefix=f"{job_id}/")
              keys = [obj["Key"] for obj in result.get("Contents", [])]

              with ThreadPoolExecutor(max_workers=32) as executor:
                  files = list(executor.map(lambda key: s3.get_object(Bucket=bucket, Key=key)["Body"], keys))

              file_iterators = [file.iter_lines() for file in files]

              # Initialize the multipart upload
              merged_key = f"{job_id}-result.txt"
              multipart_upload = s3.create_multipart_upload(Bucket=bucket, Key=merged_key)
              upload_id = multipart_upload['UploadId']
              part_number = 1
              parts = []

              # Size threshold for each part
              part_size_threshold = 32 * 2**20  # 32 MiB
              current_buffer = io.BytesIO()  # Buffer to hold data before uploading
              current_size = 0

              def upload_part():
                  nonlocal merged_key, upload_id, part_number, parts
                  nonlocal part_size_threshold, current_buffer, current_size
                  # Reset the buffer pointer to the beginning to upload the content
                  current_buffer.seek(0)

                  # Upload the buffered part to S3
                  print(f"Uploading part {part_number}, size: {current_size / 2**20} MiB")
                  part = s3.upload_part(
                      Bucket=bucket,
                      Key=merged_key,
                      PartNumber=part_number,
                      UploadId=upload_id,
                      Body=current_buffer
                  )
                  parts.append({'PartNumber': part_number, 'ETag': part['ETag']})
                  part_number += 1  # Increment part number for the next part

                  # Reset the buffer for the next part
                  current_buffer = io.BytesIO()
                  current_size = 0

              mode = event["query_mode"]
              if mode not in ['labels', 'matches']:
                  raise ValueError(f"Invalid query mode: {mode}")
              
              # Process the lines and buffer them
              for idx in it.count(0):
                  with ThreadPoolExecutor(max_workers=32) as executor:
                      lines = list(executor.map(lambda iterator: next(iterator, None), file_iterators))
                  
                  if not lines or not lines[0]:
                      break
                  Name = ''
                  Matches = []
                  for _, name, *matches in map(lambda x: x.decode().split('\t'), lines):
                      if not Name:
                          Name = name
                      assert name == Name
                      if mode == 'labels' and matches[0]:
                          Matches += matches
                      elif mode == 'matches':
                          for match in matches:
                              label, count = match.split(':')
                              Matches.append((int(count), label))

                  # Prepare the content for the current part
                  if mode == 'labels':
                      content = f"{idx}\t{Name}\t{':'.join(Matches)}\n"
                  elif mode == 'matches':
                      content = f"{idx}\t{Name}\t{'\t'.join([f'{label}:{count}' for count, label in reversed(sorted(Matches))])}\n"

                  # Write the content to the buffer
                  current_buffer.write(content.encode())
                  current_size += len(content.encode())

                  # If the buffer has reached the size threshold, upload it as a part
                  if current_size >= part_size_threshold:
                      upload_part()

              # If there is any remaining data in the buffer after processing all lines, upload it
              if current_size > 0:
                  upload_part()

              # Complete the multipart upload after all parts are uploaded
              s3.complete_multipart_upload(
                  Bucket=bucket,
                  Key=merged_key,
                  UploadId=upload_id,
                  MultipartUpload={'Parts': parts}
              )

              return {
                  "job_id": job_id,
                  "fails": event["fails"],
                  "total": len(event["tasks"]),
                  "merged_key": merged_key
              }
      Runtime: python3.13
      MemorySize: 256
      Timeout: 900

  MetagraphNotifier:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt MetagraphLambdaRole.Arn
      Runtime: python3.13
      Handler: index.lambda_handler
      Timeout: 60
      Code:
        ZipFile: !Sub |
          import boto3, os
          from botocore.config import Config
          from urllib.parse import urlparse

          sns = boto3.client("sns")
          s3 = boto3.client("s3", region_name='eu-central-2', endpoint_url='https://s3.eu-central-2.amazonaws.com')

          def lambda_handler(event, context):
              presigned_url = s3.generate_presigned_url(
                  ClientMethod='get_object',
                  Params={'Bucket': '${MetagraphBucket}', 'Key': event["merged_key"]},
                  ExpiresIn=7*24*60*60  # 7 days
              )
              print(f"Sending notification to topic ${MetagraphNotificationTopic}")
              sns.publish(
                  TopicArn="${MetagraphNotificationTopic}",
                  Message=(
                      f"Your Metagraph query {event['job_id']} has completed.\n\n"
                      f"Failed tasks: {event['fails']} out of {event['total']}\n\n"
                      f"Download your results here (valid for 7 days):\n\n{presigned_url}"
                  ),
                  Subject="Your Metagraph query has completed"
              )

  MetagraphStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      RoleArn: !GetAtt MetagraphStepFunctionsRole.Arn
      DefinitionString: !Sub |
        {
          "StartAt": "InvokeScheduler",
          "States": {
            "InvokeScheduler": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${MetagraphTaskScheduler.Arn}",
                "Payload": {
                  "arn.$": "$$.Execution.Id",
                  "query.$": "$"
                }
              },
              "Next": "WaitJobsDone"
            },
            "WaitJobsDone": {
              "Type": "Wait",
              "Seconds": 300,
              "Next": "CheckJobsStatus"
            },
            "CheckJobsStatus": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${MetagraphJobMonitor.Arn}",
                "Payload.$": "$"
              },
              "Next": "AreJobsDone"
            },
            "AreJobsDone": {
              "Type": "Choice",
              "Choices": [
                {
                  "Variable": "$.all_done",
                  "BooleanEquals": true,
                  "Next": "GatherResults"
                }
              ],
              "Default": "WaitJobsDone"
            },
            "GatherResults": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "OutputPath": "$.Payload",
              "Parameters": {
                "FunctionName": "${MetagraphResultAggregator.Arn}",
                "Payload.$": "$"
              },
              "Next": "NotifyUser"
            },
            "NotifyUser": {
              "Type": "Task",
              "Resource": "arn:aws:states:::lambda:invoke",
              "Parameters": {
                "FunctionName": "${MetagraphNotifier.Arn}",
                "Payload.$": "$"
              },
              "End": true
            }
          }
        }


Outputs:
  BucketName:
    Description: 'S3 Bucket for storing queries and their results.'
    Value: !Ref MetagraphBucket
  StateMachineArn:
    Description: 'ARN of the Step Functions state machine to trigger queries.'
    Value: !Ref MetagraphStateMachine
